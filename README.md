# ğŸ“˜ Research RAG Assistant  
A retrieval-augmented language model for academic question-answering.

This project builds a small research assistant that can read a collection of academic papers and answer questions about them. It uses a classic RAG pipeline: the documents are split into meaningful chunks, embedded into a vector store, and retrieved on demand to ground the modelâ€™s answers.

The goal is simple: give an LLM access to your papers so it can answer domain-specific questions more reliably.

## ğŸ§  What This Project Does
- Loads PDFs (papers, theses, reports) and breaks them into clean text chunks.
- Creates embeddings and stores them in a local vector database (Chroma).
- Retrieves the most relevant pieces of text for each query.
- Sends both the question and retrieved context to a language model.
- Returns grounded answers instead of hallucinations.
- Includes a minimal Streamlit interface for interaction.

## ğŸ—ï¸ Project Structure
```
rag_assistant/
â”‚
â”œâ”€â”€ data/
â”œâ”€â”€ chroma_store/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”œâ”€â”€ embed.py
â”‚   â”œâ”€â”€ retrieve.py
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ› ï¸ Installation
```bash
pip install -r requirements.txt
```

Add your OpenAI API key:
```bash
export OPENAI_API_KEY="your_key_here"
```

## ğŸ“¥ Step 1 â€” Add Your PDFs
Place PDFs inside:
```
data/
```

## ğŸ”§ Step 2 â€” Build Vector DB
```bash
python src/embed.py
```

## ğŸ” Step 3 â€” Ask Questions
```bash
python src/retrieve.py
```

Or:
```python
from retrieve import ask_question
print(ask_question("Summarize transformer fine-tuning methods."))
```

## ğŸ’¬ Streamlit UI
```bash
streamlit run src/app.py
```

## ğŸ“Š Example
**Q:** What are major challenges in applying LLMs?  
**A:** Depends on retrieved documents; typically data scarcity, evaluation, and domain adaptation issues.

## ğŸ“Œ Future Work
- Retrieval evaluation
- Different embedding models
- Metadata filters
- Deployment
